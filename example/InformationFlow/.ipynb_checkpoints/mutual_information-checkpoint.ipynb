{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b918ad-1c0f-4534-93dd-b1f695a1e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mutual_information(x, y, bins=16, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Estimate Mutual Information between x and y via 2D histograms.\n",
    "    \n",
    "    Args:\n",
    "        x, y: 1D numpy arrays of the same length.\n",
    "        bins: number of bins to use for discretization.\n",
    "        eps: small value to avoid log(0).\n",
    "\n",
    "    Returns:\n",
    "        Estimated mutual information in bits (if log base=2).\n",
    "    \"\"\"\n",
    "    # 1. Compute 2D histogram of x and y\n",
    "    counts_xy, x_edges, y_edges = np.histogram2d(x, y, bins=bins)\n",
    "    \n",
    "    # 2. Convert counts to joint probabilities\n",
    "    p_xy = counts_xy / np.sum(counts_xy)\n",
    "    \n",
    "    # 3. Compute marginals p(x), p(y)\n",
    "    p_x = np.sum(p_xy, axis=1, keepdims=True)  # sum over y dimension\n",
    "    p_y = np.sum(p_xy, axis=0, keepdims=True)  # sum over x dimension\n",
    "\n",
    "    # 4. Sum up p_xy * log( p_xy / (p_x * p_y) )\n",
    "    # We’ll use log base 2 => MI in bits.\n",
    "    # Make sure to handle zero counts properly.\n",
    "    valid = p_xy > 0\n",
    "    mi = np.sum(p_xy[valid] * np.log2( p_xy[valid] / (p_x[valid[:,0]] * p_y[0,valid[0]]) ))\n",
    "    \n",
    "    return mi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184964ae-fde2-488f-b597-35bf3fcb3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_entropy(x, y, bins=16, lag=1, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Estimate Transfer Entropy T_{X->Y} for a 1D time series x, y using \n",
    "    naive discrete histograms (lag-1).\n",
    "\n",
    "    Args:\n",
    "        x, y: 1D numpy arrays, same length T.\n",
    "        bins: number of bins for discretization.\n",
    "        lag: the time shift to measure X at time t vs. Y at time t+1.\n",
    "        eps: small offset to avoid log(0).\n",
    "\n",
    "    Returns:\n",
    "        Estimated transfer entropy from x to y in bits.\n",
    "        \n",
    "    Note:\n",
    "        T_{X->Y}(lag=1) uses data points:\n",
    "          - y_{t+1}, y_t, x_t\n",
    "          for t = 0..(T-2) if lag=1\n",
    "          (We skip the last index or so to avoid out-of-range indexing).\n",
    "    \"\"\"\n",
    "    length = len(x)\n",
    "    if len(y) != length:\n",
    "        raise ValueError(\"x and y must have the same length.\")\n",
    "    if length <= lag:\n",
    "        raise ValueError(\"Time series is too short for the given lag.\")\n",
    "\n",
    "    # We'll form arrays:\n",
    "    #   X_t   = x[0 : T-lag-1]\n",
    "    #   Y_t   = y[0 : T-lag-1]\n",
    "    #   Y_t+1 = y[1 : T-lag]\n",
    "    # Actually for lag=1: \n",
    "    #   y_{t+1} is y[t+1], \n",
    "    #   y_t is y[t], \n",
    "    #   x_t is x[t].\n",
    "    # We'll skip the final point(s) to align all.\n",
    "\n",
    "    x_t = x[:-lag-1]\n",
    "    y_t = y[:-lag-1]\n",
    "    y_next = y[1:-lag]\n",
    "\n",
    "    # 1. 3D histogram for (x_t, y_t, y_{t+1})\n",
    "    # Flatten them into shape (N,) each\n",
    "    # We'll collect them into columns for np.histogramdd\n",
    "    data_3d = np.vstack([x_t, y_t, y_next]).T  # shape (N, 3)\n",
    "    counts_3d, edges = np.histogramdd(data_3d, bins=(bins, bins, bins))\n",
    "    p_xyz = counts_3d / np.sum(counts_3d)   # joint distribution p(x,y,y_next)\n",
    "\n",
    "    # 2. We also need:\n",
    "    #    p(y,y_next), p(y,x,y_next), etc.\n",
    "    #    We'll handle them by summation over the relevant axes.\n",
    "    #    p_xy_ = sum over y_next axis\n",
    "    #    p_yy_ = sum over x axis, etc.\n",
    "\n",
    "    # a) p(y_t, x_t) = sum over y_next dimension\n",
    "    p_xy = np.sum(p_xyz, axis=2)  # shape (bins, bins)\n",
    "    # b) p(y_t, y_next) = sum over x dimension\n",
    "    p_yy = np.sum(p_xyz, axis=0)  # shape (bins, bins)\n",
    "    # c) p(y_t) = sum over x and y_next\n",
    "    p_y  = np.sum(p_yy, axis=1)   # shape (bins,)\n",
    "\n",
    "    # Transfer Entropy sum:\n",
    "    # T_{X->Y} = sum_{all} p(x_t, y_t, y_{t+1}) * log2( p(y_{t+1}| y_t, x_t) / p(y_{t+1}| y_t) )\n",
    "    # where p(y_{t+1}| y_t, x_t) = p(x_t, y_t, y_{t+1}) / p(x_t, y_t)\n",
    "    #       p(y_{t+1}| y_t)     = p(y_t, y_{t+1}) / p(y_t)\n",
    "\n",
    "    te = 0.0\n",
    "    # We'll iterate over the 3D bin indices (i_x, i_y, i_ynext)\n",
    "    # to accumulate p(x,y,y_next)*log(...) only for bins with p>0.\n",
    "    for i_x in range(bins):\n",
    "        for i_y in range(bins):\n",
    "            for i_yn in range(bins):\n",
    "                p_val = p_xyz[i_x, i_y, i_yn]\n",
    "                if p_val <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # p(x_t, y_t) is p_xy[i_x, i_y]\n",
    "                p_xy_val = p_xy[i_x, i_y]\n",
    "                if p_xy_val <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # p(y_t, y_{t+1}) is p_yy[i_y, i_yn]\n",
    "                p_yy_val = p_yy[i_y, i_yn]\n",
    "                # p(y_t) is p_y[i_y]\n",
    "                \n",
    "                # p(y_{t+1} | y_t, x_t) = p(x_t, y_t, y_{t+1}) / p(x_t, y_t)\n",
    "                p_cond1 = p_val / p_xy_val\n",
    "\n",
    "                # p(y_{t+1} | y_t) = p(y_t, y_{t+1}) / p(y_t)\n",
    "                if p_y[i_y] <= 0:\n",
    "                    continue\n",
    "                p_cond2 = p_yy_val / p_y[i_y]\n",
    "\n",
    "                # Transfer Entropy increment\n",
    "                te += p_val * np.log2((p_cond1 + eps) / (p_cond2 + eps))\n",
    "\n",
    "    return te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3f6b2bf-50be-430a-8810-39c245cc79a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (185,) (6,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m B \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mroll(A, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(N)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Just a mild shift-based coupling: B_t ~ 0.8*A_{t-1} + noise\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2. Compute mutual information\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m mi_AB \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_information\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m mi_BA \u001b[38;5;241m=\u001b[39m mutual_information(B, A, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutual Information MI(A;B) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmi_AB\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mmutual_information\u001b[0;34m(x, y, bins, eps)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 4. Sum up p_xy * log( p_xy / (p_x * p_y) )\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# We’ll use log base 2 => MI in bits.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Make sure to handle zero counts properly.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m valid \u001b[38;5;241m=\u001b[39m p_xy \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m mi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(p_xy[valid] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2( \u001b[43mp_xy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp_y\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m ))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mi\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (185,) (6,6) "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Generate synthetic data\n",
    "    np.random.seed(0)\n",
    "    N = 2000\n",
    "    A = np.random.randn(N)  # random signal\n",
    "    # B depends partly on A shifted by 1\n",
    "    B = 0.8 * np.roll(A, 1) + 0.2 * np.random.randn(N)\n",
    "    # Just a mild shift-based coupling: B_t ~ 0.8*A_{t-1} + noise\n",
    "    \n",
    "    # 2. Compute mutual information\n",
    "    mi_AB = mutual_information(A, B, bins=16)\n",
    "    mi_BA = mutual_information(B, A, bins=16)\n",
    "    \n",
    "    print(f\"Mutual Information MI(A;B) = {mi_AB:.4f} bits\")\n",
    "    print(f\"Mutual Information MI(B;A) = {mi_BA:.4f} bits (should be same)\\n\")\n",
    "    \n",
    "    # 3. Compute transfer entropy\n",
    "    te_AtoB = transfer_entropy(A, B, bins=16, lag=1)\n",
    "    te_BtoA = transfer_entropy(B, A, bins=16, lag=1)\n",
    "    \n",
    "    print(f\"Transfer Entropy T(A->B) = {te_AtoB:.4f} bits\")\n",
    "    print(f\"Transfer Entropy T(B->A) = {te_BtoA:.4f} bits\\n\")\n",
    "    \n",
    "    # 4. Interpretation\n",
    "    # - MI is symmetric, so MI(A;B) == MI(B;A).\n",
    "    # - TE can be asymmetric; T(A->B) > T(B->A) suggests A \"drives\" B more than vice versa.\n",
    "\n",
    "    # 5. (Optional) Quick example plotting for different bins\n",
    "    bins_list = [4, 8, 16, 32, 64]\n",
    "    te_results_AtoB = []\n",
    "    te_results_BtoA = []\n",
    "    \n",
    "    for b in bins_list:\n",
    "        te_results_AtoB.append(transfer_entropy(A, B, bins=b, lag=1))\n",
    "        te_results_BtoA.append(transfer_entropy(B, A, bins=b, lag=1))\n",
    "    \n",
    "    plt.plot(bins_list, te_results_AtoB, marker='o', label='T(A->B)')\n",
    "    plt.plot(bins_list, te_results_BtoA, marker='o', label='T(B->A)')\n",
    "    plt.xlabel('Number of Bins')\n",
    "    plt.ylabel('Transfer Entropy (bits)')\n",
    "    plt.title('TE vs. Binning for Synthetic A->B Coupling')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e4bcba-2469-4f3a-8f64-4a1f48337e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mutual_info_score\n",
    "def discretize(data, nbins=16):\n",
    "    \"\"\"\n",
    "    Discretize continuous 1D data into 'nbins' bins, returning\n",
    "    an array of integer bin indices (0 to nbins-1).\n",
    "    \"\"\"\n",
    "    # Find bin edges over the range of data\n",
    "    data_min, data_max = np.min(data), np.max(data)\n",
    "    # If data is constant, just return zeros\n",
    "    if data_min == data_max:\n",
    "        return np.zeros_like(data, dtype=int)\n",
    "    \n",
    "    bin_edges = np.linspace(data_min, data_max, nbins + 1)\n",
    "    # Digitize data into bins (each data point -> bin index)\n",
    "    # np.digitize returns bin indices in [1..nbins], so subtract 1 to get [0..nbins-1]\n",
    "    bin_indices = np.digitize(data, bin_edges) - 1\n",
    "    \n",
    "    # Ensure we stay within 0..nbins-1\n",
    "    bin_indices = np.clip(bin_indices, 0, nbins-1)\n",
    "    return bin_indices\n",
    "\n",
    "def mutual_info_sklearn(x, y, nbins=16):\n",
    "    \"\"\"\n",
    "    Compute mutual information between x and y by:\n",
    "      1) Discretizing x and y into nbins.\n",
    "      2) Using sklearn.metrics.mutual_info_score on the discrete labels.\n",
    "\n",
    "    Returns:\n",
    "      MI(x,y) in 'natural units' of mutual_info_score. \n",
    "      By default, mutual_info_score returns the MI in 'nats' if using np.log,\n",
    "      but effectively it's just an unnormalized number. \n",
    "      (Sklearn's docs mention it's based on the NMI formula, but let's \n",
    "       treat it as a comparative measure.)\n",
    "    \"\"\"\n",
    "    x_disc = discretize(x, nbins)\n",
    "    y_disc = discretize(y, nbins)\n",
    "    mi_value = mutual_info_score(x_disc, y_disc)\n",
    "    return mi_value\n",
    "\n",
    "def measure_mi_sliding_window_lag(x, y, window_size=100, lags=None, nbins=16):\n",
    "    \"\"\"\n",
    "    Computes mutual information between X and a time-lagged Y in a sliding-window approach.\n",
    "\n",
    "    Args:\n",
    "        x, y : 1D numpy arrays of the same length.\n",
    "        window_size : number of samples per sliding window.\n",
    "        lags : list of integer lags (>= 0) to consider.\n",
    "        nbins : number of bins for discretization in the MI calculation.\n",
    "\n",
    "    Returns:\n",
    "        - mi_matrix: 2D numpy array of shape (num_t, len(lags)), \n",
    "                     where num_t is the number of valid window positions in x.\n",
    "                     mi_matrix[t_idx, lag_idx] = MI(x_window, y_window_lagged).\n",
    "        - valid_t: a list of the starting time indices for which we compute MI.\n",
    "    \"\"\"\n",
    "    if lags is None:\n",
    "        lags = [0, 1, 2, 5, 10, 20]\n",
    "\n",
    "    length = len(x)\n",
    "    assert len(y) == length, \"x and y must have the same length.\"\n",
    "\n",
    "    # The largest lag might reduce the maximum t we can use\n",
    "    max_lag = max(lags)\n",
    "    # We'll allow windows up to length - window_size - max_lag\n",
    "    num_t = length - window_size - max_lag\n",
    "    if num_t <= 0:\n",
    "        raise ValueError(\"Time series too short for the given window_size and max_lag.\")\n",
    "    \n",
    "    mi_matrix = np.zeros((num_t, len(lags)), dtype=float)\n",
    "    valid_t = list(range(num_t))  # 0..(num_t-1) in terms of start index\n",
    "    \n",
    "    for i, t in enumerate(valid_t):\n",
    "        # X window: x[t : t+window_size]\n",
    "        x_win = x[t : t + window_size]\n",
    "        \n",
    "        for j, lag in enumerate(lags):\n",
    "            # Y window (lagged by 'lag'): y[t+lag : t+lag + window_size]\n",
    "            y_start = t + lag\n",
    "            y_win = y[y_start : y_start + window_size]\n",
    "            \n",
    "            mi_val = mutual_info_sklearn(x_win, y_win, nbins=nbins)\n",
    "            mi_matrix[i, j] = mi_val\n",
    "    \n",
    "    return mi_matrix, valid_t\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Generate synthetic data\n",
    "    np.random.seed(0)\n",
    "    N = 2000\n",
    "    A = np.random.randn(N)  # random signal\n",
    "    # B depends partly on A shifted by 1\n",
    "    B = 0.8 * np.roll(A, 1) + 0.2 * np.random.randn(N)\n",
    "    # There's a mild shift-based coupling: B_t ~ 0.8*A_{t-1} + noise\n",
    "    print(B.shape)\n",
    "    print(A.shape)\n",
    "    # 2. Define parameters for the sliding window approach\n",
    "    window_size = 100\n",
    "    lags = [0, 1, 2, 5, 10, 20, 50]  # pick lags of interest\n",
    "    nbins = 16\n",
    "    \n",
    "    # 3. Compute sliding-window MI\n",
    "    mi_matrix, valid_t = measure_mi_sliding_window_lag(\n",
    "        A, B, window_size=window_size, lags=lags, nbins=nbins\n",
    "    )\n",
    "    \n",
    "    # mi_matrix has shape (num_t, len(lags))\n",
    "    # valid_t is the list of start indices in A for which we computed MI\n",
    "    \n",
    "    # 4. Plot the MI matrix as a heatmap:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # y-axis is t index, x-axis is lag index\n",
    "    # we might want to invert or transpose for a more intuitive view\n",
    "    plt.imshow(mi_matrix.T, aspect='auto', origin='lower',\n",
    "               extent=[valid_t[0], valid_t[-1], lags[0], lags[-1]])\n",
    "    plt.colorbar(label='Mutual Info (in nats via sklearn)')\n",
    "    plt.xlabel('Sliding window start index t')\n",
    "    plt.ylabel('Lag')\n",
    "    plt.title('Sliding-Window Mutual Information (A vs. B lagged)')\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Example: Plot MI vs. lag averaged across time\n",
    "    mi_mean_over_t = np.mean(mi_matrix, axis=0)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(lags, mi_mean_over_t, marker='o')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Mean MI over time windows')\n",
    "    plt.title('Average MI(A, B_lagged) over all sliding windows')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175a487-1f2a-4be8-a592-78662cbaaadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
