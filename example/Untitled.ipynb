{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980fa69-c7fc-498b-8302-b43b3b4f6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage, stats\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.spatial.distance as dist\n",
    "from scipy.signal import correlate2d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SpatioTemporalInfoAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive toolkit for quantifying information content in evolving 2D fields\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, field_data, dx=1.0, dt=1.0):\n",
    "        \"\"\"\n",
    "        field_data: 3D array (time, y, x) representing the evolving 2D field\n",
    "        dx, dt: spatial and temporal resolution\n",
    "        \"\"\"\n",
    "        self.data = np.array(field_data)\n",
    "        self.T, self.ny, self.nx = self.data.shape\n",
    "        self.dx = dx\n",
    "        self.dt = dt\n",
    "        \n",
    "    def differential_entropy_kde(self, field_slice, bandwidth=None):\n",
    "        \"\"\"\n",
    "        Estimate differential entropy using kernel density estimation\n",
    "        \"\"\"\n",
    "        field_flat = field_slice.flatten()\n",
    "        # Remove any NaN or inf values\n",
    "        field_flat = field_flat[np.isfinite(field_flat)]\n",
    "        \n",
    "        if len(field_flat) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Estimate PDF using KDE\n",
    "        try:\n",
    "            kde = stats.gaussian_kde(field_flat, bw_method=bandwidth)\n",
    "            # Evaluate KDE on a grid\n",
    "            x_grid = np.linspace(field_flat.min(), field_flat.max(), 100)\n",
    "            pdf_vals = kde(x_grid)\n",
    "            pdf_vals = pdf_vals[pdf_vals > 1e-10]  # Avoid log(0)\n",
    "            \n",
    "            # Numerical integration for entropy\n",
    "            dx_grid = x_grid[1] - x_grid[0] if len(x_grid) > 1 else 1.0\n",
    "            entropy = -np.sum(pdf_vals * np.log(pdf_vals)) * dx_grid\n",
    "            return entropy\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def spatial_mutual_information(self, t1, t2, lag_x=1, lag_y=1, bins=50):\n",
    "        \"\"\"\n",
    "        Calculate mutual information between spatially separated regions\n",
    "        \"\"\"\n",
    "        field1 = self.data[t1]\n",
    "        field2 = self.data[t2]\n",
    "        \n",
    "        # Create spatially lagged versions\n",
    "        region1 = field1[:-lag_y, :-lag_x].flatten()\n",
    "        region2 = field2[lag_y:, lag_x:].flatten()\n",
    "        \n",
    "        # Remove invalid values\n",
    "        valid_mask = np.isfinite(region1) & np.isfinite(region2)\n",
    "        region1 = region1[valid_mask]\n",
    "        region2 = region2[valid_mask]\n",
    "        \n",
    "        if len(region1) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate mutual information using binning\n",
    "        try:\n",
    "            hist_2d, x_edges, y_edges = np.histogram2d(region1, region2, bins=bins)\n",
    "            hist_2d = hist_2d + 1e-10  # Avoid log(0)\n",
    "            \n",
    "            # Normalize to get probabilities\n",
    "            p_xy = hist_2d / np.sum(hist_2d)\n",
    "            p_x = np.sum(p_xy, axis=1)\n",
    "            p_y = np.sum(p_xy, axis=0)\n",
    "            \n",
    "            # Calculate MI\n",
    "            mi = 0.0\n",
    "            for i in range(len(p_x)):\n",
    "                for j in range(len(p_y)):\n",
    "                    if p_xy[i,j] > 0 and p_x[i] > 0 and p_y[j] > 0:\n",
    "                        mi += p_xy[i,j] * np.log(p_xy[i,j] / (p_x[i] * p_y[j]))\n",
    "            \n",
    "            return mi\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def transfer_entropy_estimator(self, source_ts, target_ts, lag=1, bins=10):\n",
    "        \"\"\"\n",
    "        Estimate transfer entropy between two time series\n",
    "        TE(X->Y) = I(Y_t+1; X_t | Y_t)\n",
    "        \"\"\"\n",
    "        if len(source_ts) != len(target_ts) or len(source_ts) < lag + 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Prepare variables for TE calculation\n",
    "        y_future = target_ts[lag+1:]\n",
    "        x_past = source_ts[lag:-1]\n",
    "        y_past = target_ts[lag:-1]\n",
    "        \n",
    "        # Remove invalid values\n",
    "        valid_mask = np.isfinite(y_future) & np.isfinite(x_past) & np.isfinite(y_past)\n",
    "        y_future = y_future[valid_mask]\n",
    "        x_past = x_past[valid_mask]\n",
    "        y_past = y_past[valid_mask]\n",
    "        \n",
    "        if len(y_future) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Calculate conditional mutual information\n",
    "            # TE = I(Y_t+1; X_t | Y_t) = H(Y_t+1 | Y_t) - H(Y_t+1 | X_t, Y_t)\n",
    "            \n",
    "            # Bin the data\n",
    "            y_fut_binned = np.digitize(y_future, np.linspace(y_future.min(), y_future.max(), bins))\n",
    "            x_past_binned = np.digitize(x_past, np.linspace(x_past.min(), x_past.max(), bins))\n",
    "            y_past_binned = np.digitize(y_past, np.linspace(y_past.min(), y_past.max(), bins))\n",
    "            \n",
    "            # Calculate joint and marginal entropies\n",
    "            def joint_entropy_2d(x, y):\n",
    "                hist, _, _ = np.histogram2d(x, y, bins=bins)\n",
    "                hist = hist + 1e-10\n",
    "                p = hist / np.sum(hist)\n",
    "                return -np.sum(p * np.log(p))\n",
    "            \n",
    "            def joint_entropy_3d(x, y, z):\n",
    "                # Simplified 3D entropy calculation\n",
    "                combined = x * bins**2 + y * bins + z\n",
    "                unique, counts = np.unique(combined, return_counts=True)\n",
    "                p = counts / np.sum(counts)\n",
    "                p = p + 1e-10\n",
    "                return -np.sum(p * np.log(p))\n",
    "            \n",
    "            h_y_fut_y_past = joint_entropy_2d(y_future, y_past)\n",
    "            h_y_past = -np.sum((np.bincount(y_past_binned) / len(y_past_binned)) * \n",
    "                              np.log(np.bincount(y_past_binned) / len(y_past_binned) + 1e-10))\n",
    "            \n",
    "            h_y_fut_x_past_y_past = joint_entropy_3d(y_fut_binned, x_past_binned, y_past_binned)\n",
    "            h_x_past_y_past = joint_entropy_2d(x_past, y_past)\n",
    "            \n",
    "            # TE = I(Y_fut; X_past | Y_past)\n",
    "            te = h_y_fut_y_past + h_x_past_y_past - h_y_fut_x_past_y_past - h_y_past\n",
    "            \n",
    "            return max(0, te)  # TE should be non-negative\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def entropy_rate(self, window_size=5):\n",
    "        \"\"\"\n",
    "        Calculate entropy rate - new information per time step\n",
    "        \"\"\"\n",
    "        rates = []\n",
    "        for t in range(window_size, self.T):\n",
    "            current_entropy = self.differential_entropy_kde(self.data[t])\n",
    "            \n",
    "            # Conditional entropy given past window\n",
    "            past_window = self.data[t-window_size:t]\n",
    "            past_info = np.mean([self.differential_entropy_kde(frame) for frame in past_window])\n",
    "            \n",
    "            # Rate is new entropy beyond what's predictable from past\n",
    "            rate = current_entropy - 0.8 * past_info  # 0.8 is a damping factor\n",
    "            rates.append(max(0, rate))\n",
    "        \n",
    "        return np.array(rates)\n",
    "    \n",
    "    def information_flow_field(self, lag=1):\n",
    "        \"\"\"\n",
    "        Calculate information flow between neighboring spatial regions\n",
    "        Returns a 2D field showing local information transfer rates\n",
    "        \"\"\"\n",
    "        flow_field = np.zeros((self.ny-2, self.nx-2))\n",
    "        \n",
    "        for i in range(1, self.ny-1):\n",
    "            for j in range(1, self.nx-1):\n",
    "                # Extract time series for center pixel and neighbors\n",
    "                center_ts = self.data[:, i, j]\n",
    "                \n",
    "                # Calculate average transfer entropy from neighbors\n",
    "                neighbors = [\n",
    "                    self.data[:, i-1, j],    # up\n",
    "                    self.data[:, i+1, j],    # down\n",
    "                    self.data[:, i, j-1],    # left\n",
    "                    self.data[:, i, j+1],    # right\n",
    "                ]\n",
    "                \n",
    "                te_values = []\n",
    "                for neighbor_ts in neighbors:\n",
    "                    te = self.transfer_entropy_estimator(neighbor_ts, center_ts, lag)\n",
    "                    te_values.append(te)\n",
    "                \n",
    "                flow_field[i-1, j-1] = np.mean(te_values)\n",
    "        \n",
    "        return flow_field\n",
    "    \n",
    "    def wavefront_info_velocity(self):\n",
    "        \"\"\"\n",
    "        Estimate the velocity of information propagation\n",
    "        \"\"\"\n",
    "        velocities = []\n",
    "        \n",
    "        for t in range(1, self.T-1):\n",
    "            # Calculate spatial gradients of information\n",
    "            info_field = np.abs(self.data[t+1] - self.data[t-1]) / (2 * self.dt)\n",
    "            \n",
    "            # Estimate wavefront using edge detection\n",
    "            grad_x = np.gradient(info_field, self.dx, axis=1)\n",
    "            grad_y = np.gradient(info_field, self.dx, axis=0)\n",
    "            \n",
    "            # Information velocity magnitude\n",
    "            velocity_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            velocities.append(np.mean(velocity_mag[np.isfinite(velocity_mag)]))\n",
    "        \n",
    "        return np.array(velocities)\n",
    "    \n",
    "    def analyze_complete(self):\n",
    "        \"\"\"\n",
    "        Perform comprehensive spatio-temporal information analysis\n",
    "        \"\"\"\n",
    "        print(\"Performing comprehensive spatio-temporal information analysis...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. Temporal entropy evolution\n",
    "        print(\"  - Calculating temporal entropy evolution...\")\n",
    "        temporal_entropies = []\n",
    "        for t in range(self.T):\n",
    "            entropy = self.differential_entropy_kde(self.data[t])\n",
    "            temporal_entropies.append(entropy)\n",
    "        results['temporal_entropy'] = np.array(temporal_entropies)\n",
    "        \n",
    "        # 2. Entropy rate\n",
    "        print(\"  - Calculating entropy rate...\")\n",
    "        results['entropy_rate'] = self.entropy_rate()\n",
    "        \n",
    "        # 3. Spatial mutual information evolution\n",
    "        print(\"  - Calculating spatial correlations...\")\n",
    "        spatial_mi = []\n",
    "        for t in range(self.T-1):\n",
    "            mi = self.spatial_mutual_information(t, t+1, lag_x=1, lag_y=1)\n",
    "            spatial_mi.append(mi)\n",
    "        results['spatial_mi'] = np.array(spatial_mi)\n",
    "        \n",
    "        # 4. Information flow field\n",
    "        print(\"  - Calculating information flow field...\")\n",
    "        results['flow_field'] = self.information_flow_field()\n",
    "        \n",
    "        # 5. Information velocity\n",
    "        print(\"  - Estimating information propagation velocity...\")\n",
    "        results['info_velocity'] = self.wavefront_info_velocity()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_analysis(self, results):\n",
    "        \"\"\"\n",
    "        Visualize the information analysis results\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Original field evolution (show first, middle, last frames)\n",
    "        times_to_show = [0, self.T//2, self.T-1]\n",
    "        for idx, t in enumerate(times_to_show):\n",
    "            ax = axes[0, idx]\n",
    "            im = ax.imshow(self.data[t], cmap='viridis', aspect='auto')\n",
    "            ax.set_title(f'Field at t={t}')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "        \n",
    "        # Temporal entropy evolution\n",
    "        axes[1, 0].plot(results['temporal_entropy'])\n",
    "        axes[1, 0].set_title('Temporal Entropy Evolution')\n",
    "        axes[1, 0].set_xlabel('Time')\n",
    "        axes[1, 0].set_ylabel('Differential Entropy')\n",
    "        \n",
    "        # Entropy rate\n",
    "        axes[1, 1].plot(results['entropy_rate'])\n",
    "        axes[1, 1].set_title('Information Generation Rate')\n",
    "        axes[1, 1].set_xlabel('Time')\n",
    "        axes[1, 1].set_ylabel('Entropy Rate')\n",
    "        \n",
    "        # Information flow field\n",
    "        im = axes[1, 2].imshow(results['flow_field'], cmap='plasma', aspect='auto')\n",
    "        axes[1, 2].set_title('Information Flow Field')\n",
    "        plt.colorbar(im, ax=axes[1, 2])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\n=== SPATIO-TEMPORAL INFORMATION ANALYSIS SUMMARY ===\")\n",
    "        print(f\"Total entropy change: {results['temporal_entropy'][-1] - results['temporal_entropy'][0]:.3f}\")\n",
    "        print(f\"Average entropy rate: {np.mean(results['entropy_rate']):.3f}\")\n",
    "        print(f\"Peak information generation: {np.max(results['entropy_rate']):.3f}\")\n",
    "        print(f\"Average spatial MI: {np.mean(results['spatial_mi']):.3f}\")\n",
    "        print(f\"Mean information flow: {np.mean(results['flow_field']):.3f}\")\n",
    "        print(f\"Average information velocity: {np.mean(results['info_velocity']):.3f}\")\n",
    "\n",
    "# Example usage with simulated data\n",
    "def generate_example_field():\n",
    "    \"\"\"Generate a sample evolving 2D field (wave equation solution)\"\"\"\n",
    "    nx, ny, nt = 50, 50, 100\n",
    "    dx, dy, dt = 0.1, 0.1, 0.01\n",
    "    \n",
    "    x = np.linspace(0, 5, nx)\n",
    "    y = np.linspace(0, 5, ny)\n",
    "    t = np.linspace(0, 1, nt)\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    field_data = np.zeros((nt, ny, nx))\n",
    "    \n",
    "    # Simulate a wave propagating with some nonlinearity\n",
    "    for i, time in enumerate(t):\n",
    "        # Multiple wave sources with different frequencies\n",
    "        wave1 = np.sin(2*np.pi*(X - 2*time)) * np.exp(-((X-2.5)**2 + (Y-2.5)**2)/2)\n",
    "        wave2 = 0.5*np.sin(3*np.pi*(Y - 1.5*time)) * np.exp(-((X-1)**2 + (Y-4)**2)/1)\n",
    "        \n",
    "        # Add some nonlinear interaction\n",
    "        field_data[i] = wave1 + wave2 + 0.1*wave1*wave2\n",
    "        \n",
    "        # Add small amount of noise\n",
    "        field_data[i] += 0.05 * np.random.randn(ny, nx)\n",
    "    \n",
    "    return field_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate example data\n",
    "    print(\"Generating example 2D evolving field...\")\n",
    "    field_data = generate_example_field()\n",
    "    \n",
    "    # Create analyzer\n",
    "    analyzer = SpatioTemporalInfoAnalyzer(field_data, dx=0.1, dt=0.01)\n",
    "    \n",
    "    # Perform analysis\n",
    "    results = analyzer.analyze_complete()\n",
    "    \n",
    "    # Visualize results\n",
    "    analyzer.plot_analysis(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8dce3-509f-47e8-81b3-c0ec6c71b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PersistentHomologyInfoAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze information flow using persistent homology of evolving 2D fields\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, field_data, threshold_method='percentile'):\n",
    "        \"\"\"\n",
    "        field_data: 3D array (time, y, x)\n",
    "        threshold_method: 'percentile', 'adaptive', or 'fixed'\n",
    "        \"\"\"\n",
    "        self.data = np.array(field_data)\n",
    "        self.T, self.ny, self.nx = self.data.shape\n",
    "        self.threshold_method = threshold_method\n",
    "        \n",
    "    def compute_sublevel_sets(self, field_slice, thresholds):\n",
    "        \"\"\"\n",
    "        Compute sublevel sets for different threshold values\n",
    "        Returns binary masks for each threshold\n",
    "        \"\"\"\n",
    "        sublevel_sets = []\n",
    "        for thresh in thresholds:\n",
    "            sublevel = field_slice <= thresh\n",
    "            sublevel_sets.append(sublevel)\n",
    "        return sublevel_sets\n",
    "    \n",
    "    def compute_superlevel_sets(self, field_slice, thresholds):\n",
    "        \"\"\"\n",
    "        Compute superlevel sets for different threshold values\n",
    "        More relevant for information flow (high activity regions)\n",
    "        \"\"\"\n",
    "        superlevel_sets = []\n",
    "        for thresh in thresholds:\n",
    "            superlevel = field_slice >= thresh\n",
    "            superlevel_sets.append(superlevel)\n",
    "        return superlevel_sets\n",
    "    \n",
    "    def connected_components_2d(self, binary_mask):\n",
    "        \"\"\"\n",
    "        Find connected components in 2D binary mask\n",
    "        Returns component labels and count\n",
    "        \"\"\"\n",
    "        labeled, num_components = ndimage.label(binary_mask)\n",
    "        return labeled, num_components\n",
    "    \n",
    "    def find_loops_2d(self, binary_mask):\n",
    "        \"\"\"\n",
    "        Simplified loop detection using Euler characteristic\n",
    "        For 2D: χ = V - E + F, loops create holes (reduce χ)\n",
    "        \"\"\"\n",
    "        labeled, num_components = self.connected_components_2d(binary_mask)\n",
    "        \n",
    "        if num_components == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Euler characteristic for each component\n",
    "        total_loops = 0\n",
    "        for comp_id in range(1, num_components + 1):\n",
    "            component = (labeled == comp_id)\n",
    "            \n",
    "            # Count vertices (pixels), edges, and estimate loops\n",
    "            vertices = np.sum(component)\n",
    "            if vertices < 4:  # Too small for a loop\n",
    "                continue\n",
    "                \n",
    "            # Estimate edges by counting neighbor connections\n",
    "            edges = 0\n",
    "            for i in range(self.ny-1):\n",
    "                for j in range(self.nx-1):\n",
    "                    if component[i,j]:\n",
    "                        # Count connections to neighbors\n",
    "                        if j < self.nx-1 and component[i,j+1]:\n",
    "                            edges += 1\n",
    "                        if i < self.ny-1 and component[i+1,j]:\n",
    "                            edges += 1\n",
    "            \n",
    "            # For 2D grid: χ = 1 - loops (for connected component)\n",
    "            # Rough approximation: loops ≈ edges - vertices + 1\n",
    "            component_loops = max(0, edges - vertices + 1)\n",
    "            total_loops += component_loops\n",
    "        \n",
    "        return total_loops\n",
    "    \n",
    "    def persistence_diagram(self, field_slice, level_type='super'):\n",
    "        \"\"\"\n",
    "        Compute persistence diagram for a single field slice\n",
    "        Returns birth-death pairs for H0 (components) and H1 (loops)\n",
    "        \"\"\"\n",
    "        # Determine threshold range\n",
    "        field_min, field_max = np.min(field_slice), np.max(field_slice)\n",
    "        \n",
    "        if self.threshold_method == 'percentile':\n",
    "            thresholds = np.percentile(field_slice.flatten(), np.linspace(5, 95, 20))\n",
    "        elif self.threshold_method == 'adaptive':\n",
    "            mean_val = np.mean(field_slice)\n",
    "            std_val = np.std(field_slice)\n",
    "            thresholds = np.linspace(mean_val - 2*std_val, mean_val + 2*std_val, 20)\n",
    "        else:  # fixed\n",
    "            thresholds = np.linspace(field_min, field_max, 20)\n",
    "        \n",
    "        thresholds = np.sort(thresholds)\n",
    "        \n",
    "        # Compute level sets\n",
    "        if level_type == 'super':\n",
    "            level_sets = self.compute_superlevel_sets(field_slice, thresholds)\n",
    "        else:\n",
    "            level_sets = self.compute_sublevel_sets(field_slice, thresholds)\n",
    "        \n",
    "        # Track components and loops across thresholds\n",
    "        h0_births = []  # Component births\n",
    "        h0_deaths = []  # Component deaths\n",
    "        h1_births = []  # Loop births\n",
    "        h1_deaths = []  # Loop deaths\n",
    "        \n",
    "        prev_components = 0\n",
    "        prev_loops = 0\n",
    "        component_tracker = {}\n",
    "        loop_tracker = {}\n",
    "        \n",
    "        for i, (thresh, level_set) in enumerate(zip(thresholds, level_sets)):\n",
    "            # Count components and loops\n",
    "            _, num_components = self.connected_components_2d(level_set)\n",
    "            num_loops = self.find_loops_2d(level_set)\n",
    "            \n",
    "            # Track component births and deaths\n",
    "            if num_components > prev_components:\n",
    "                # New components born\n",
    "                for _ in range(num_components - prev_components):\n",
    "                    h0_births.append(thresh)\n",
    "            elif num_components < prev_components:\n",
    "                # Components died\n",
    "                for _ in range(prev_components - num_components):\n",
    "                    h0_deaths.append(thresh)\n",
    "            \n",
    "            # Track loop births and deaths\n",
    "            if num_loops > prev_loops:\n",
    "                for _ in range(num_loops - prev_loops):\n",
    "                    h1_births.append(thresh)\n",
    "            elif num_loops < prev_loops:\n",
    "                for _ in range(prev_loops - num_loops):\n",
    "                    h1_deaths.append(thresh)\n",
    "            \n",
    "            prev_components = num_components\n",
    "            prev_loops = num_loops\n",
    "        \n",
    "        # Match births with deaths to create persistence pairs\n",
    "        h0_pairs = self._match_births_deaths(h0_births, h0_deaths, thresholds[-1])\n",
    "        h1_pairs = self._match_births_deaths(h1_births, h1_deaths, thresholds[-1])\n",
    "        \n",
    "        return {\n",
    "            'H0': h0_pairs,  # Connected components\n",
    "            'H1': h1_pairs,  # Loops/holes\n",
    "            'thresholds': thresholds\n",
    "        }\n",
    "    \n",
    "    def _match_births_deaths(self, births, deaths, max_thresh):\n",
    "        \"\"\"\n",
    "        Match birth and death events to create persistence pairs\n",
    "        \"\"\"\n",
    "        births = sorted(births)\n",
    "        deaths = sorted(deaths)\n",
    "        \n",
    "        pairs = []\n",
    "        \n",
    "        # Match deaths with most recent births\n",
    "        birth_idx = 0\n",
    "        for death in deaths:\n",
    "            if birth_idx < len(births):\n",
    "                pairs.append((births[birth_idx], death))\n",
    "                birth_idx += 1\n",
    "        \n",
    "        # Remaining births persist to infinity (max threshold)\n",
    "        while birth_idx < len(births):\n",
    "            pairs.append((births[birth_idx], max_thresh))\n",
    "            birth_idx += 1\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def persistence_landscape(self, persistence_pairs, resolution=100):\n",
    "        \"\"\"\n",
    "        Convert persistence diagram to persistence landscape\n",
    "        More stable representation for statistical analysis\n",
    "        \"\"\"\n",
    "        if not persistence_pairs:\n",
    "            return np.zeros(resolution), np.linspace(0, 1, resolution)\n",
    "        \n",
    "        # Extract birth and death times\n",
    "        births = [pair[0] for pair in persistence_pairs]\n",
    "        deaths = [pair[1] for pair in persistence_pairs]\n",
    "        \n",
    "        if not births:\n",
    "            return np.zeros(resolution), np.linspace(0, 1, resolution)\n",
    "        \n",
    "        min_val = min(births)\n",
    "        max_val = max(deaths)\n",
    "        \n",
    "        if max_val == min_val:\n",
    "            return np.zeros(resolution), np.linspace(min_val, min_val + 1, resolution)\n",
    "        \n",
    "        x = np.linspace(min_val, max_val, resolution)\n",
    "        landscape = np.zeros(resolution)\n",
    "        \n",
    "        # For each persistence pair, add triangle function\n",
    "        for birth, death in persistence_pairs:\n",
    "            persistence = death - birth\n",
    "            midpoint = (birth + death) / 2\n",
    "            \n",
    "            # Create triangle function\n",
    "            for i, xi in enumerate(x):\n",
    "                if birth <= xi <= death:\n",
    "                    if xi <= midpoint:\n",
    "                        landscape[i] += (xi - birth) / (midpoint - birth) * persistence / 2\n",
    "                    else:\n",
    "                        landscape[i] += (death - xi) / (death - midpoint) * persistence / 2\n",
    "        \n",
    "        return landscape, x\n",
    "    \n",
    "    def bottleneck_distance(self, pairs1, pairs2):\n",
    "        \"\"\"\n",
    "        Compute bottleneck distance between two persistence diagrams\n",
    "        Simplified version using maximum matching\n",
    "        \"\"\"\n",
    "        if not pairs1 and not pairs2:\n",
    "            return 0.0\n",
    "        if not pairs1 or not pairs2:\n",
    "            # Distance to empty diagram\n",
    "            all_pairs = pairs1 if pairs1 else pairs2\n",
    "            return max([abs(death - birth) for birth, death in all_pairs])\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        distances = []\n",
    "        for b1, d1 in pairs1:\n",
    "            min_dist = float('inf')\n",
    "            for b2, d2 in pairs2:\n",
    "                dist = max(abs(b1 - b2), abs(d1 - d2))\n",
    "                min_dist = min(min_dist, dist)\n",
    "            distances.append(min_dist)\n",
    "        \n",
    "        return max(distances) if distances else 0.0\n",
    "    \n",
    "    def information_flow_velocity_topological(self):\n",
    "        \"\"\"\n",
    "        Estimate information flow velocity using topological feature tracking\n",
    "        \"\"\"\n",
    "        velocities = []\n",
    "        \n",
    "        for t in range(self.T - 1):\n",
    "            # Get persistence diagrams for consecutive frames\n",
    "            pd1 = self.persistence_diagram(self.data[t])\n",
    "            pd2 = self.persistence_diagram(self.data[t + 1])\n",
    "            \n",
    "            # Track component centroid movement\n",
    "            component_velocities = []\n",
    "            \n",
    "            # Get component centroids for both frames\n",
    "            _, components1 = self.connected_components_2d(self.data[t] > np.percentile(self.data[t], 75))\n",
    "            _, components2 = self.connected_components_2d(self.data[t+1] > np.percentile(self.data[t+1], 75))\n",
    "            \n",
    "            centroids1 = []\n",
    "            for comp_id in range(1, components1 + 1):\n",
    "                mask = (ndimage.label(self.data[t] > np.percentile(self.data[t], 75))[0] == comp_id)\n",
    "                if np.sum(mask) > 0:\n",
    "                    cy, cx = ndimage.center_of_mass(mask)\n",
    "                    centroids1.append((cx, cy))\n",
    "            \n",
    "            centroids2 = []\n",
    "            for comp_id in range(1, components2 + 1):\n",
    "                mask = (ndimage.label(self.data[t+1] > np.percentile(self.data[t+1], 75))[0] == comp_id)\n",
    "                if np.sum(mask) > 0:\n",
    "                    cy, cx = ndimage.center_of_mass(mask)\n",
    "                    centroids2.append((cx, cy))\n",
    "            \n",
    "            # Match centroids and compute velocities\n",
    "            if centroids1 and centroids2:\n",
    "                for c1 in centroids1:\n",
    "                    min_dist = float('inf')\n",
    "                    best_match = None\n",
    "                    for c2 in centroids2:\n",
    "                        dist = np.sqrt((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2)\n",
    "                        if dist < min_dist:\n",
    "                            min_dist = dist\n",
    "                            best_match = c2\n",
    "                    \n",
    "                    if best_match and min_dist < 10:  # Reasonable matching threshold\n",
    "                        velocity = min_dist  # Distance per time step\n",
    "                        component_velocities.append(velocity)\n",
    "            \n",
    "            avg_velocity = np.mean(component_velocities) if component_velocities else 0\n",
    "            velocities.append(avg_velocity)\n",
    "        \n",
    "        return np.array(velocities)\n",
    "    \n",
    "    def topological_complexity_evolution(self):\n",
    "        \"\"\"\n",
    "        Track evolution of topological complexity over time\n",
    "        \"\"\"\n",
    "        h0_complexity = []  # Component complexity\n",
    "        h1_complexity = []  # Loop complexity\n",
    "        bottleneck_distances = []\n",
    "        \n",
    "        prev_pd = None\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            pd = self.persistence_diagram(self.data[t])\n",
    "            \n",
    "            # Complexity measures\n",
    "            h0_pers = [death - birth for birth, death in pd['H0']]\n",
    "            h1_pers = [death - birth for birth, death in pd['H1']]\n",
    "            \n",
    "            h0_complexity.append(np.sum(h0_pers))\n",
    "            h1_complexity.append(np.sum(h1_pers))\n",
    "            \n",
    "            # Bottleneck distance to previous frame\n",
    "            if prev_pd is not None:\n",
    "                bd_h0 = self.bottleneck_distance(prev_pd['H0'], pd['H0'])\n",
    "                bd_h1 = self.bottleneck_distance(prev_pd['H1'], pd['H1'])\n",
    "                bottleneck_distances.append(bd_h0 + bd_h1)\n",
    "            \n",
    "            prev_pd = pd\n",
    "        \n",
    "        return {\n",
    "            'h0_complexity': np.array(h0_complexity),\n",
    "            'h1_complexity': np.array(h1_complexity),\n",
    "            'bottleneck_distances': np.array(bottleneck_distances),\n",
    "            'total_complexity': np.array(h0_complexity) + np.array(h1_complexity)\n",
    "        }\n",
    "    \n",
    "    def information_creation_annihilation(self):\n",
    "        \"\"\"\n",
    "        Identify regions where topological information is created/destroyed\n",
    "        \"\"\"\n",
    "        creation_map = np.zeros((self.T-1, self.ny, self.nx))\n",
    "        annihilation_map = np.zeros((self.T-1, self.ny, self.nx))\n",
    "        \n",
    "        for t in range(self.T - 1):\n",
    "            # Get high-activity regions for both frames\n",
    "            thresh1 = np.percentile(self.data[t], 80)\n",
    "            thresh2 = np.percentile(self.data[t+1], 80)\n",
    "            \n",
    "            active1 = self.data[t] > thresh1\n",
    "            active2 = self.data[t+1] > thresh2\n",
    "            \n",
    "            # Information creation: new active regions\n",
    "            creation = active2 & ~active1\n",
    "            creation_map[t] = creation.astype(float)\n",
    "            \n",
    "            # Information annihilation: lost active regions\n",
    "            annihilation = active1 & ~active2\n",
    "            annihilation_map[t] = annihilation.astype(float)\n",
    "        \n",
    "        return creation_map, annihilation_map\n",
    "    \n",
    "    def analyze_complete_topological(self):\n",
    "        \"\"\"\n",
    "        Complete topological analysis of information flow\n",
    "        \"\"\"\n",
    "        print(\"Performing topological information flow analysis...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. Topological complexity evolution\n",
    "        print(\"  - Computing topological complexity evolution...\")\n",
    "        results['complexity'] = self.topological_complexity_evolution()\n",
    "        \n",
    "        # 2. Information flow velocity\n",
    "        print(\"  - Estimating topological flow velocity...\")\n",
    "        results['topo_velocity'] = self.information_flow_velocity_topological()\n",
    "        \n",
    "        # 3. Information creation/annihilation maps\n",
    "        print(\"  - Mapping information creation/annihilation...\")\n",
    "        creation_map, annihilation_map = self.information_creation_annihilation()\n",
    "        results['creation_map'] = creation_map\n",
    "        results['annihilation_map'] = annihilation_map\n",
    "        \n",
    "        # 4. Persistence landscapes for representative frames\n",
    "        print(\"  - Computing persistence landscapes...\")\n",
    "        representative_frames = [0, self.T//4, self.T//2, 3*self.T//4, self.T-1]\n",
    "        results['landscapes'] = {}\n",
    "        \n",
    "        for frame in representative_frames:\n",
    "            pd = self.persistence_diagram(self.data[frame])\n",
    "            landscape_h0, x_h0 = self.persistence_landscape(pd['H0'])\n",
    "            landscape_h1, x_h1 = self.persistence_landscape(pd['H1'])\n",
    "            \n",
    "            results['landscapes'][frame] = {\n",
    "                'H0': (landscape_h0, x_h0),\n",
    "                'H1': (landscape_h1, x_h1)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_topological_analysis(self, results):\n",
    "        \"\"\"\n",
    "        Visualize topological analysis results\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        # 1. Original field evolution\n",
    "        ax1 = plt.subplot(3, 4, 1)\n",
    "        im1 = ax1.imshow(self.data[0], cmap='viridis')\n",
    "        ax1.set_title('Initial Field')\n",
    "        plt.colorbar(im1, ax=ax1)\n",
    "        \n",
    "        ax2 = plt.subplot(3, 4, 2)\n",
    "        im2 = ax2.imshow(self.data[self.T//2], cmap='viridis')\n",
    "        ax2.set_title('Mid-evolution Field')\n",
    "        plt.colorbar(im2, ax=ax2)\n",
    "        \n",
    "        ax3 = plt.subplot(3, 4, 3)\n",
    "        im3 = ax3.imshow(self.data[-1], cmap='viridis')\n",
    "        ax3.set_title('Final Field')\n",
    "        plt.colorbar(im3, ax=ax3)\n",
    "        \n",
    "        # 2. Topological complexity evolution\n",
    "        ax4 = plt.subplot(3, 4, 4)\n",
    "        ax4.plot(results['complexity']['h0_complexity'], label='H0 (Components)', linewidth=2)\n",
    "        ax4.plot(results['complexity']['h1_complexity'], label='H1 (Loops)', linewidth=2)\n",
    "        ax4.plot(results['complexity']['total_complexity'], label='Total', linewidth=2, linestyle='--')\n",
    "        ax4.set_title('Topological Complexity Evolution')\n",
    "        ax4.set_xlabel('Time')\n",
    "        ax4.set_ylabel('Persistence Sum')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "        \n",
    "        # 3. Bottleneck distances (topological change rate)\n",
    "        ax5 = plt.subplot(3, 4, 5)\n",
    "        if len(results['complexity']['bottleneck_distances']) > 0:\n",
    "            ax5.plot(results['complexity']['bottleneck_distances'], 'r-', linewidth=2)\n",
    "        ax5.set_title('Topological Change Rate')\n",
    "        ax5.set_xlabel('Time')\n",
    "        ax5.set_ylabel('Bottleneck Distance')\n",
    "        ax5.grid(True)\n",
    "        \n",
    "        # 4. Information flow velocity\n",
    "        ax6 = plt.subplot(3, 4, 6)\n",
    "        if len(results['topo_velocity']) > 0:\n",
    "            ax6.plot(results['topo_velocity'], 'g-', linewidth=2)\n",
    "        ax6.set_title('Topological Flow Velocity')\n",
    "        ax6.set_xlabel('Time')\n",
    "        ax6.set_ylabel('Velocity')\n",
    "        ax6.grid(True)\n",
    "        \n",
    "        # 5. Information creation map (average)\n",
    "        ax7 = plt.subplot(3, 4, 7)\n",
    "        creation_avg = np.mean(results['creation_map'], axis=0)\n",
    "        im7 = ax7.imshow(creation_avg, cmap='Reds')\n",
    "        ax7.set_title('Information Creation Map')\n",
    "        plt.colorbar(im7, ax=ax7)\n",
    "        \n",
    "        # 6. Information annihilation map (average)\n",
    "        ax8 = plt.subplot(3, 4, 8)\n",
    "        annihilation_avg = np.mean(results['annihilation_map'], axis=0)\n",
    "        im8 = ax8.imshow(annihilation_avg, cmap='Blues')\n",
    "        ax8.set_title('Information Annihilation Map')\n",
    "        plt.colorbar(im8, ax=ax8)\n",
    "        \n",
    "        # 7-8. Persistence landscapes\n",
    "        ax9 = plt.subplot(3, 4, 9)\n",
    "        ax10 = plt.subplot(3, 4, 10)\n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "        for i, (frame, color) in enumerate(zip(results['landscapes'].keys(), colors)):\n",
    "            if frame in results['landscapes']:\n",
    "                landscape_h0, x_h0 = results['landscapes'][frame]['H0']\n",
    "                if len(landscape_h0) > 0:\n",
    "                    ax9.plot(x_h0, landscape_h0, color=color, label=f't={frame}', alpha=0.7)\n",
    "                \n",
    "                landscape_h1, x_h1 = results['landscapes'][frame]['H1']\n",
    "                if len(landscape_h1) > 0:\n",
    "                    ax10.plot(x_h1, landscape_h1, color=color, label=f't={frame}', alpha=0.7)\n",
    "        \n",
    "        ax9.set_title('H0 Persistence Landscapes')\n",
    "        ax9.set_xlabel('Threshold')\n",
    "        ax9.set_ylabel('Landscape Value')\n",
    "        ax9.legend()\n",
    "        ax9.grid(True)\n",
    "        \n",
    "        ax10.set_title('H1 Persistence Landscapes')\n",
    "        ax10.set_xlabel('Threshold')\n",
    "        ax10.set_ylabel('Landscape Value')\n",
    "        ax10.legend()\n",
    "        ax10.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n=== TOPOLOGICAL INFORMATION ANALYSIS SUMMARY ===\")\n",
    "        print(f\"Peak topological complexity: {np.max(results['complexity']['total_complexity']):.3f}\")\n",
    "        print(f\"Average complexity change rate: {np.mean(results['complexity']['bottleneck_distances']):.3f}\")\n",
    "        print(f\"Average topological flow velocity: {np.mean(results['topo_velocity']):.3f}\")\n",
    "        print(f\"Total information creation events: {np.sum(results['creation_map']):.0f}\")\n",
    "        print(f\"Total information annihilation events: {np.sum(results['annihilation_map']):.0f}\")\n",
    "\n",
    "# Example usage\n",
    "def generate_complex_field():\n",
    "    \"\"\"Generate a field with rich topological structure\"\"\"\n",
    "    nx, ny, nt = 60, 60, 80\n",
    "    x = np.linspace(0, 6, nx)\n",
    "    y = np.linspace(0, 6, ny)\n",
    "    t = np.linspace(0, 2, nt)\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    field_data = np.zeros((nt, ny, nx))\n",
    "    \n",
    "    for i, time in enumerate(t):\n",
    "        # Multiple interacting waves creating complex topology\n",
    "        wave1 = np.sin(2*np.pi*(X - 1.5*time)) * np.exp(-((X-3)**2 + (Y-3)**2)/3)\n",
    "        wave2 = np.sin(3*np.pi*(Y - 1.2*time)) * np.exp(-((X-1)**2 + (Y-5)**2)/2)\n",
    "        wave3 = np.cos(1.5*np.pi*((X-2*time)**2 + (Y-1.5*time)**2)) * np.exp(-((X-4)**2 + (Y-2)**2)/4)\n",
    "        \n",
    "        # Nonlinear interactions create loops and complex structures\n",
    "        field = wave1 + wave2 + wave3\n",
    "        field += 0.3 * wave1 * wave2 + 0.2 * wave2 * wave3\n",
    "        \n",
    "        # Add localized \"vortices\" that create topological features\n",
    "        vortex1 = np.sin(4*time) * np.exp(-((X-2)**2 + (Y-4)**2)/1.5)\n",
    "        vortex2 = np.cos(3*time) * np.exp(-((X-5)**2 + (Y-1)**2)/1.5)\n",
    "        \n",
    "        field += 0.5 * (vortex1 + vortex2)\n",
    "        \n",
    "        # Small noise\n",
    "        field += 0.1 * np.random.randn(ny, nx)\n",
    "        \n",
    "        field_data[i] = field\n",
    "    \n",
    "    return field_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate example with rich topological structure\n",
    "    print(\"Generating complex field with topological features...\")\n",
    "    field_data = generate_complex_field()\n",
    "    \n",
    "    # Create analyzer\n",
    "    analyzer = PersistentHomologyInfoAnalyzer(field_data, threshold_method='percentile')\n",
    "    \n",
    "    # Perform analysis\n",
    "    results = analyzer.analyze_complete_topological()\n",
    "    \n",
    "    # Visualize\n",
    "    analyzer.plot_topological_analysis(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
